* Introduction

Exo aims to facilitate incremental development with a fast
edit-to-test cycle for systems where integration testing cannot easily
be performed without real world external I/O.  And just to be clear,
since everyone does web servers these days... I'm not talking about
I/O in terms of web requests.  The original context for this project
development of embedded systems that interact more directly with the
physical world.  This is usually paired with inacessibility and
heterogeneity of platforms.

In this problem domain, a testing and scaffolding system is usually
designed together with the system under test.  Exo is a tool library
and a set of design principles that took shape over time while
designing such scaffolding systems, and also taking incremental
development and deployment serious in situations where bad tooling is
typically just tolerated.

Exo is built primarily on two principles: a declarative build system
with incremental change propagation, and a set of tools that
generalize this approach from just dealing with software binaries,
into a problem space where management of distristributed state becomes
a problem that derves some respect.

Exo takes the distributed, always-on system ideas of Erlang and
generalizes them to polyglot, heterogeneous systems, and to support a
much finer granularity when upgrading code.

The end game is to completely remove the ideas of compilation and
deployment, and make the code reflect the running system at all times,
keeping the downtime to a minimum.

In practice this turns out to be tremendously difficult to do in the
heterogeneous setting, especially when some of the "services" in the
network run on bare-bones microcontrollers.

The main goal of the Exo project as a software repository is to fork
off libraries and design documentation about how to create systems
with this in mind.

The main technical issues seem to concentrate around:

   - distribution: communication processes distributed across
     networks.

   - code change and propagation: development is incremental. managing
     cached data is a hurdle (e.g. compilation, deployment, partial
     system restarts).

   - binding: software components across machines need to be able to
     find each other.  the core problem here is mapping identities to
     routable addresses.


* Status

The Exo project is a synthesis of many ideas that are simple and
straightforward in isolation.  The usefulness of this project is in
how these ideas are combined.

I have arrived at a working collection of tools and design principles.
What is missing is packaging of subcomponents, stabilization of APIs,
code cleanup, code robustness and high level documentation.

I currently experience a lot of trouble linearizing the documentation
to explain how these parts interact.  If things are unclear to you,
that will be on me, so do not hesitate to ask questions or send
remarks.

The name of the project is derived from the name "exocortex"[1].
While this project is not about AI or brain-computer interfaces, I
find the analogy still useful.  This project is about integration,
which I believe to be the essence of "brain" once you project it onto
boring, feasible software engineering.

Exo's first couple of iterations were aimed at just integrating
devices and services that run on my personal network (thermostat,
video cameras, entertainment systems, ...), but it quickly turned
meta, focusing on supporting my main activity which is _incremental
development_ of such an integrated system.  I currently do all my
other software development in an always-on fashion, integrated into
one monolith.

[1] https://futurehumanity.wordpress.com/2012/09/09/exocortex/

* Exo pilars

This section aims at describing some of the ideas and software
patterns used in Exo.

** Incremental improvement, refactoring

With all systems gradually turning into distributed systems, software
development has gotten even more complex.  While writing solid modular
components is still the best way to manage this, integrating those
into larger systems is a difficult process.  I know I am going to get
the APIs wrong if I try to design them.  So I have settled on taking a
more organic, incremental approach, where systems integration layers
and submodules are built at the same time, focusing on happy path and
gradually growing that way.  All successful large systems started out
as successful small systems.

The main idea here is to make it easy to make small changes.

In Exo, the incremental development approach is built on the ideas
behind redo, re-implemented in Erlang.  Redo is a declarative build
system that uses a clever trick to discover dependencies.

Exo was originally built around apenwarr's shell-based redo which in
turn was based on djb's specification, but it was apparent that a more
natural language embedding would be necessary to remove some of the
hurdles.

Redo is declarative wrapper around mutable code.  Most real-world
systems are mutable.  Redo can act as a clever bridge between two
worlds. Apenwarr's blog posts about redo go into this more deeply.

Implementing redo in Erlang allowed two important improvements.  One
is to make it easier to handle a more abstract idea of state as
opposed to just managing files in the filesystem, and the ability to
better structure build rules by expressing them through Erlang's
pattern matching mechanism.

One of the abstract pieces of state that an Erlang system can manage
is process state, which can be generalized by proxy to any kind of
distributed system state.  E.g. state in a database, or some process
running on some small microcontroller.

A second improvement is to use Erlang's pattern matching functionality
to express build rules.  This allows rule specifications to be Erlang
data structures, removing the hassle of trying to encode variant
information in file names.



This idea is not new.  It is essentially continuous integration.  I do
want to stress that the important difference is the much finer
granularity of code changes, compilation, deployment and partial
restarts.  The aim is to get updates down to millisecond levels to
truly make the difference between source code an a running system
impercievable.  Avoiding mental context switches is key.


** Differences of the redo implementation vs. filesystem redo

1. It allows Erlang data structures to be used as names.  This is
   convenient for pattern matching.

2. Focusing on structured names makes name translation functions a
   valuable abstraction mechanism.

3. Separating names and abstract storage is very convenient.  A
   filesystem interface doesn't capture everything, unfortunately.

4. Piggy backing on Erlang multi-processing is straightforward.
   Erlang can be used as process monitor for opaque state services.


To do this with files, you would have to use name mangling, and some
alternative files system <-> opaque state



** Idiosyncratic structure

Since I had to start somewhere, and my problem domain is fairly niche
(distributed embedded software), I will be un unapologetically
idiosyncratic.

The system has had a lot of evolutionary pressure based on how my
brain works, i.e. how I forget and remember things as I continue
aging.  I am currently not entirely sure how much of that is just
quirks, and how much can be distilled into a more universal approach.

I do have an intention to move Exo from the current incubator stage
into a more useful form that fits consensus reality.  The reality is
that packaging is a tremendous amount of work, so most code slowly
moves through stages.  Currently the reusable parts go into the public
erl_tools library.


** Discoverability

Exo code uses an approach that I believe is called "discoverable code
patterns".  It is based on the idea that documentation is very
expensive, and that in many cases it is really better to not create
paper documentation, but to first make the code base more
discoverable.  This is done by leaving "bread crumbs", pointers to
places to start reading code.  These can then be used later to be
pointed to from paper documentation.

I found that this approach is assisted tremendously by the ability to
perform incremental code edits on a running system.  I.e. to learn the
system, you "nudge" the running system into a different regime by
editing it while it is running.  Recognize Smalltalk, but extended to
any kind of heterogeneous platform.

A typical process of re-familiarizing myself with a design is to
uncomment log statements directly in the code, instead of having
infrastructure for enabling/disabling log statements through
configuration variables.  Every developer I know works like that.  In
a system that can be edited when it is live this is very
straightfoward to do this, and it avoids the complexity of creating a
separate logging configuration system.

Not having configurable log infrastructure is an example of
intentionally _not_ building an abstraction.  A typical pitfall of
application development is developer feature creep, i.e. to build in
too much infrastructure that is just aimed at developers.  I am still
doing that, obviously, but in a way that is part of the meta system,
not the subcomponent/library/application itself.


** Distributed Systems

Due to the physical component of the work I do (embedded software),
any system is necessarily distributed.  This creates a lot of
problems.  So much in fact that the core design of Exo is built around
dealing with the non-locality.  It uses Erlang's distribution system
as a backbone, and any leaf/edge node interfaces are extended in the
same fashion by setting up Erlang proxy processes that bridge a leaf
node's messaging system into the shared Exo space.  The abstraction
that is distilled from this is epid from erl_tools.


** Simplicity

Systems are getting too complex to understand.  This is turning into a
real problem.

For Exo, I aim at simplicity of implementation.  Focus on happy path,
use fault-tolerant principles to deal with and discover failure modes,
and re-implement existing ideas without the bells and whistles of
highly configurable libraries.

I also try to aim at simplicity of features.  My assessment is that
feature creep and feature duplications are problems to be avoided at
all cost.  This ties into the idea that subcomponents need to be built
while being part of an integrated system.

** Dependencies are a liability

As part of the focus on simplicity is the realization that code reuse
does not always have a positive effect and in today's landscape often
doesn't.  It is hard to find reusable components that are just right,
and do not introduce their own unwieldly dependency tree.

Bringing in dependencies adds integration problems, maintenance issues
due to upstream bugs, and generally code bloat due to duplication
across dependnecies.

Obviously there is a tradeoff here, and it is very much conditional on
the structure of the development team.  For Exo's top layer, there is
a team of one, and this strongly nudges the requirement towards
simplicity first, which often means to boil down and rewrite.

Exo's reusable library layer is _not_ written with a team of one in
mind.  Basically, I want to understand both what is good for me
personally working in isolation, _and_ what is good in a collaborative
context.  In the latter context, the Exo spin-offs result in simple
libraries that can be reused in other projects that each can walk
their own inevitable path of feature creep on an as-needed basis.

This approach is also used for contract work: I start out integrating
a client's system into Exo, and then gradually cut the umbilical
towards a simple, self-contained system.

** Functions and Processes

Exo is heavily built on the ideas that underlie Erlang: use (pure)
functional code wherever possible, and gradually introduce processes
(distributed objects) as real-world constraints start making this a
necessity.

Pure Functions (and the dual, pure data), are about composition, and
composition/refactoring of functions and data is the most important
tool in the programmer's toolbox.

After all these years it still regularly amazes me how good of a
one-size-fits-all abstraction function composition really is, and how
difficult it is to internalize this and trust it to guide almost every
design decision.


** A note on types.

In the context of Exo, there is an important line to be drawn between
dynamically typed and statically typed code.  It has become more clear
over time that there is a tradeoff here, and that both paradigms are
useful.  There is a skill to learn about how to move around the grain
boundary between the two.

My current assessment is that the main reason to use static types is
to facilitate maintenance of complex projects.  The main reason not to
is to implement Smalltalk/Lisp/Erlang-like systems like Exo that are
intended to be modified on the fly: changing types on the fly is not
possible in a statically typed system, but can be made to work if you
have a layer of dynamic type interpretation.

Code that has stabilized can and probably should be moved from one end
to the other, and strongly typed code is easier to develop when
integrated in a more fluid framework or test jig.

Exo contains interfaces for incremental development of C, Rust and
Haskell code.


** Composite names and the connectivity problem

This is a design principle that is difficult to explain because its
effects on code structure are indirect.

Composite naming is important to get right, as it can greatly
simplify code that does grouping across any of the component name
spaces.

I've run into this when structuring build rules in the Erlang redo
system.  It also pops up frequently in database schema design, where
composite keys provide the bridge between relations and functions.

Another way to put a similar idea: it is an art to design algebraic
data structures such that the functionality implemented over them is
factored properly.  The interplay is between the code and the data
structures.

Related to naming redo targets such that build rules can be expressed
using pattern matching.


** Multihop routing / source routing

This is the idea of using composite naming to solve message routing
problems.

A multihop address is a list of nodes, where a singleton list
specifies the destination as reachable from the last router, and any
address prepended to the list is the locally routable address of the
router that can deal with the rest of the multi-hop address.

Multi-hop addresses make it easy to solve routing without the need for
distributing routing tables.

In networks that do not change topology frequently, multi-hop routing
can be used in a two-step fasion: map an identitiy to a multihop
address in a possibly expensive discovery procedure, and use the
multihop address to then perform communication.

Exo uses multi-hop addresses to provide very fine identity
granularity.  The first element in a multi-hop address is an Erlang
process that can then interpret the rest of the address to forward to
some subsystem.


** Routing, multipath and path optimization

In many networked applications it is often important to distinguish
control plane and data plane, where the control plain is a
low-performance but flexible messaging system that is used to set up
optimized high-bandwidth connections.

In Exo this is typically Erlang messaging that is being used to set up
a connection, which then exchanges data through another medium.
E.g. a direct TCP connection, or local shared memory or IPC.

One example is routing MIDI controllers in Exo: It can be transported
over Erlang messages, but when both end points are on the same Jack
daemon, the Jack port connection mechanism can be used.


** Binding, name resolution

Already hinted at above.  In distributed systems, name resolution can
become a real problem if not designed properly.  This can be identity
to routable address mapping, or any other form of compilation from
specification to implementation.

In exo this is solved using two abstractions: mapping of names to
epids (routable addresses), and the use of a distributed
highly-available store.

TODO: Relate this to DHT and magnet links.


** Names create the network

Related to the previous section.

TODO: This is not explained well. Document the final idea -- global
namespace -- instead of the idea that it isnt hard to insert
global->local name translations.

Naming is the tool that implements module interaction.  The trick here
is that if you keep name lookup abstract, you can start out with no
name lookup at all, i.e. keeping naming scheme identical at both ends.
Often this tells you that naming doesn't need to be abstracted
further, and that you have an opportunity to simplify right there.
I.e. focusing on naming makes integration simpler, and allows
simplification during integration.  Put differently: every time you
have a name translation step, you can ask the question: can I refactor
to eliminate the need, and standardize the naming scheme?

The big lesson is to prefer a namespace that is shared.  The
interesting tension here is that while most software engineering
prefers locality or distributed architecture over centralized
architecture, naming is really different.  Essentially, name
resolution servce the purpose of a lingua franca.


** Is it possible to agree on a global naming scheme?

Every time someone introduces a global scheme, someone else adds a
source routing tag to it, i.e. a "wrapper".  Is it possible to solve
this problem by assuming it is impossible to create a root in the
first place?

This hints at the diea that the naming topology we want is a network,
not a tree.  Each node can still have it's local spanning tree view of
the network, i.e. where the local naming node is picked as the naming
root.


** Service architecture

What Erlang does right is to make basic inter-process communication
simple, and to provide a design template for managing (partial)
failures of distributed systems.  These two problems pop up in every
distributed system, and dealing with them is usually where most of the
development time is spent.

This also ties into naming, reconnecting across node failures, and
possibly re-routing when one node goes down and appears somewhere
else.


** Link to DevOps systems

There are many parallells with DevOps / deployment systems.  Some
inspiration comes from the ideas behind systems like Puppet, Chef,
Nix.  Apart from some experience with Nix I am not very familiar with
the structure and problems of these systems.


** Creating a place

One thing I've noticed once I started focusing consciously on
incremental code development is that it helps to give code a
"place". There's a tension between the need to create abstract library
code (a recipe), and its instantiation as something that resembles a
physical thing.

I like the messy desk analogy. The eventual product does not include
the messy desk, but the desk (incubator, scaffolding code, Exo) is an
essential element to make the code real. Things started to improve
dramatically for me once I started treating the messy desk with
respect.

This might be biased strongly to my own experience, but I've seen
similar ideas expressed by others.  There is something about human
cognition that makes it easier to manipulate concepts that have a
physical analogy as opposed to ideas.  It's as if more of the
hard-wired part of the brain can be used if you can translate a
problem into something that has a special / mechanical analogy.

I was reminded of this once I started reassembling my analog music
studio, taking a long break from a very long period of just doing
programming work at the keyboard.  Thinking about signal routing
became so obvious once it was became tangible.

Manipulating objects in an abstract space seems is seems to somehow
still be mediated through a spacial / mechanical view of the universe.
The typical example is manipulation of code or algebraic expressions
through the proxy of seeing them as a physical object with degrees of
freedom.



** Why is scaffolding code so thick?

It bridges the simple, platonic ideal in one's head, without which
nobody would do any software development, with the actual reality of
dotted i's and crossed t's.

Glue code and boiler plate is a necessity because real world systems
are complex, and handle many more use cases than just your simplified
view of the world.

It's good to have that layer as an explicit adapter such that the
world on the inside can be kept simple and malleable.


** Erlang as a backbone

Why does this work so well?  Erlang is at a sweet spot.  Its data
structures are fixed, yet powerful enough to embed almost any
protocol.  The data structure are a base line that enable hot code
reload, straightforward serialization and distribution, and structured
routing based on pattern matching, both for message dispatch (epids)
and for e.g. build rule dispatch as used in the redo abstraction that
takes center stage in Exo.
